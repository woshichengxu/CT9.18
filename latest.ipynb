{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589caa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "f = zipfile.ZipFile(\"./Image_NII.zip\",'r') # 原压缩文件在服务器的位置\n",
    "for file in f.namelist():\n",
    "    f.extract(file,\"./Image_NII\")               # 解压到的位置\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4490eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from model_34 import generate_model\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "#from utils.logger import log\n",
    "import os\n",
    "#from test_CT import test\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "from setting_CT_test import parse_opts\n",
    "from CT_preprocessing_no import CTDataset\n",
    "import random\n",
    "from sklearn.metrics import precision_score,f1_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15039026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe9a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6577c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    Image.MAX_IMAGE_PIXELS = 1000000000000000\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "setup_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a57111",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = parse_opts()\n",
    "# 删除隐藏文件/文件夹\n",
    "for root, dirs, files in os.walk('./trails'):\n",
    "    for file in files:\n",
    "        if 'ipynb_checkpoints' in file:\n",
    "            os.remove(os.path.join(root, file))\n",
    "    if 'ipynb_checkpoints' in root:\n",
    "        shutil.rmtree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a53d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets.pretrain_path=r'C:/Users/abc/Desktop/MedicalNet_pytorch_files/pretrain/resnet_34_23dataset.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518acfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, parameters = generate_model(sets)\n",
    "criterion = nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "#num_ftrs = model.module.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, 10)\n",
    "#model.fc = nn.Sequential(\n",
    "    #nn.Linear(num_ftrs, 4),\n",
    "    #nn.ReLU(),\n",
    "    #nn.Dropout(0.4),\n",
    "    #nn.Linear(224, 4),\n",
    "    #nn.LogSoftmax(dim=1)\n",
    "#)\n",
    "\n",
    "params = [\n",
    "    {'params': parameters['base_parameters'], 'lr': sets.learning_rate},\n",
    "    {'params': parameters['new_parameters'], 'lr': sets.learning_rate * 100}\n",
    "    ]\n",
    "device=torch.device('cuda')#注意一下这个跑了之后似乎网络有一点变化\n",
    "model.to(DEVICE)\n",
    "# 选择简单暴力的Adam优化器，学习率调低\n",
    "optimizer = torch.optim.Adam(params, betas=(0.9, 0.999), eps=1e-3)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634019da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_loader, model, optimizer, scheduler, total_epochs, save_interval, save_folder, sets):\n",
    "    scheduler.step()\n",
    "    loss_train,labels_train,output_train = [],[],[]\n",
    "    for batch_id, batch_data in enumerate(train_data_loader):\n",
    "        volumes, labels_ji = batch_data\n",
    "        labeltry=labels_ji.tolist()\n",
    "        for i in range(len(labeltry)):\n",
    "            labels_train.append(labeltry[i])\n",
    "        if not sets.no_cuda:\n",
    "            volumes = volumes.cuda()\n",
    "            labels = labels_ji.cuda().long()\n",
    "        im = Variable(volumes)\n",
    "        output = model(volumes)\n",
    "        loss_value_cls = criterion(output, labels)\n",
    "        loss = loss_value_cls\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            # print_loss = loss.data.item()\n",
    "            # sum_loss += print_loss\n",
    "        real_loss=loss.item()\n",
    "        realloss=real_loss*len(batch_data)\n",
    "        loss_train.append(realloss)\n",
    "        out_t1 = output.argmax(dim=1)  # 取出预测的最大值\n",
    "            #result.append(out_t1)\n",
    "        out_t = out_t1.cpu().tolist()\n",
    "        for i in range(len(out_t)):\n",
    "            output_train.append(out_t[i])\n",
    "            #print('Train Epoch: {} [{}/{} ({:.0f}%)]'.format(epoch, (batch_id + 1) * len(volumes), len(train_data_loader.dataset),100. * (batch_id + 1) / len(train_data_loader)))\n",
    "        # print('epoch:{}, train_acc: {}, loss:{},precision:{}'.format(epoch, f1, recall,precision))\n",
    "            \n",
    "    return labels_train,output_train,loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6dc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_data_loader, model, sets):\n",
    "    # batch_id_sp = epoch * batches_per_epoch\n",
    "    model.eval()\n",
    "    labels_val,out_val,val_loss=[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(val_data_loader):\n",
    "            # data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "            volumes, label_val = batch_data\n",
    "            labelval=label_val.tolist()\n",
    "            for i in range(len(labelval)):\n",
    "                labels_val.append(labelval[i])\n",
    "            if not sets.no_cuda:\n",
    "                volumes = volumes.cuda()\n",
    "                label = label_val.cuda().long()\n",
    "            output = model(volumes)\n",
    "            # output.load_state_dict(checkpoint['state_dict'])#\n",
    "            loss_value_cls = criterion(output, label)\n",
    "            loss = loss_value_cls\n",
    "            val_loss.append(loss.item())\n",
    "            y_result = output.argmax(dim=1)\n",
    "            y_result = y_result.cpu().tolist()\n",
    "            for i in range(len(y_result)):\n",
    "                out_val.append(y_result[i])\n",
    "            #print('val Epoch: {} [{}/{} ({:.0f}%)]'.format(epoch, (batch_id + 1) * len(volumes), len(val_data_loader.dataset),100. * (batch_id + 1) / len(val_data_loader)))\n",
    "\n",
    "    return labels_val,out_val,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae947c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_loader, model, sets):\n",
    "    # batch_id_sp = epoch * batches_per_epoch\n",
    "    model.eval()\n",
    "    labels_test,out_test=[],[]\n",
    "    for batch_id, batch_data in enumerate(test_data_loader):\n",
    "            # data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        volumes, label_test = batch_data\n",
    "        labeltest=label_test.tolist()\n",
    "        for i in range(len(labeltest)):\n",
    "            labels_test.append(labeltest[i])\n",
    "        if not sets.no_cuda:\n",
    "            volumes = volumes.cuda()\n",
    "            label = label_test.cuda().long()\n",
    "            output = model(volumes)\n",
    "            # output.load_state_dict(checkpoint['state_dict'])#\n",
    "            out=output.argmax(dim=1)\n",
    "            out = out.cpu().tolist()\n",
    "            for i in range(len(out)):\n",
    "                out_test.append(out[i])\n",
    "    return labels_test,out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf7e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sets.resume_path:\n",
    "    if os.path.isfile(sets.resume_path):\n",
    "        print(\"=> loading checkpoint '{}'\".format(sets.resume_path))\n",
    "        checkpoint = torch.load(sets.resume_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(sets.resume_path, checkpoint['epoch']))\n",
    "\n",
    "# getting data\n",
    "sets.phase = 'train'\n",
    "if sets.no_cuda:\n",
    "    sets.pin_memory = False\n",
    "else:\n",
    "    sets.pin_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5ab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets.batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31ba6990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model C:/Users/abc/Desktop/MedicalNet_pytorch_files/pretrain/resnet_34_23dataset.pth\n",
      "it's training in fold_0_0\n",
      "Processing 81 datas\n",
      "train len is: 81\n",
      "Processing 27 datas\n",
      "Train Epoch1:f1: 0.184481\trecall: 0.215260\tprecision: 0.161409\tloss: 3.134249\n",
      "Val Epoch:f1: 0.000000\trecall: 0.000000\tprecision: 0.000000\tloss: 7.283732\n",
      "Train Epoch2:f1: 0.189044\trecall: 0.201698\tprecision: 0.194481\tloss: 12.140556\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.690448\n",
      "Train Epoch3:f1: 0.155556\trecall: 0.225649\tprecision: 0.133990\tloss: 3.029834\n",
      "Val Epoch:f1: 0.152381\trecall: 0.333333\tprecision: 0.098765\tloss: 1.200107\n",
      "Train Epoch4:f1: 0.194602\trecall: 0.212488\tprecision: 0.181250\tloss: 2.822312\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.298242\n",
      "Train Epoch5:f1: 0.138889\trecall: 0.214286\tprecision: 0.102740\tloss: 2.699800\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.265309\n",
      "Train Epoch6:f1: 0.195767\trecall: 0.236688\tprecision: 0.166932\tloss: 2.607613\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.282138\n",
      "Train Epoch7:f1: 0.155556\trecall: 0.225649\tprecision: 0.133990\tloss: 2.653834\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.251415\n",
      "Train Epoch8:f1: 0.150862\trecall: 0.250000\tprecision: 0.108025\tloss: 2.609829\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.373756\n",
      "Train Epoch9:f1: 0.184783\trecall: 0.228247\tprecision: 0.157346\tloss: 2.712165\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.235159\n",
      "Train Epoch10:f1: 0.150862\trecall: 0.250000\tprecision: 0.108025\tloss: 2.675910\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.292247\n",
      "Train Epoch11:f1: 0.172845\trecall: 0.208117\tprecision: 0.148611\tloss: 2.602281\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.230125\n",
      "Train Epoch12:f1: 0.150862\trecall: 0.250000\tprecision: 0.108025\tloss: 2.626902\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.279485\n",
      "Train Epoch13:f1: 0.150862\trecall: 0.250000\tprecision: 0.108025\tloss: 2.598030\n",
      "Val Epoch:f1: 0.205128\trecall: 0.333333\tprecision: 0.148148\tloss: 1.224610\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39msets\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39msets\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[0;32m     32\u001b[0m                          pin_memory\u001b[38;5;241m=\u001b[39msets\u001b[38;5;241m.\u001b[39mpin_memory)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     labels,output,loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_intervals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msets\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     36\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(labels, output, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m     precision \u001b[38;5;241m=\u001b[39m precision_score(labels, output, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_data_loader, model, optimizer, scheduler, total_epochs, save_interval, save_folder, sets)\u001b[0m\n\u001b[0;32m     22\u001b[0m realloss\u001b[38;5;241m=\u001b[39mreal_loss\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch_data)\n\u001b[0;32m     23\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mappend(realloss)\n\u001b[1;32m---> 24\u001b[0m out_t1 \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 取出预测的最大值\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#result.append(out_t1)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m out_t \u001b[38;5;241m=\u001b[39m out_t1\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs=sets.n_epochs\n",
    "        \n",
    "for i in range(5):\n",
    "    for d in range(4):\n",
    "        sets.img_list_train=[]\n",
    "        sets.img_list_val=[]\n",
    "        sets.img_list_test=[]\n",
    "        training_dataset=[]\n",
    "        val_dataset=[]\n",
    "        test_dataset=[]\n",
    "        f1_val_list=[]\n",
    "        torch.manual_seed(sets.manual_seed)\n",
    "        #model, parameters = generate_model(sets)\n",
    "        model,parameters = generate_model(sets)\n",
    "        #params = [{ 'params': parameters['base_parameters'], 'lr': sets.learning_rate },{ 'params': parameters['new_parameters'], 'lr': sets.learning_rate*100 }]\n",
    "        criterion = nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "        optimizer = torch.optim.Adam(parameters, betas=(0.9, 0.999), eps=1e-2)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "        device=torch.device('cuda')#注意一下这个跑了之后似乎网络有一点变化\n",
    "        model.to(DEVICE)\n",
    "        # 选择简单暴力的Adam优化器，学习率调低\n",
    "        sets.img_list_train=('./data_list/train_fold{}f{}.txt'.format(i,d))\n",
    "        sets.img_list_val = ('./data_list/val_fold{}f{}.txt'.format(i,d))\n",
    "        sets.img_list_test = ('./data_list/test_fold{}f{}.txt'.format(i,d))\n",
    "        print(\"it's training in fold_{}_{}\".format(i,d))\n",
    "        training_dataset = CTDataset(sets.data_root, sets.img_list_train, sets, phase='train')\n",
    "        print('train len is:',len(training_dataset))\n",
    "        train_data_loader = DataLoader(training_dataset, batch_size=sets.batch_size, shuffle=True, num_workers=sets.num_workers,\n",
    "                                 pin_memory=sets.pin_memory)\n",
    "        val_dataset = CTDataset(sets.data_root, sets.img_list_val, sets, phase='train')\n",
    "        val_data_loader = DataLoader(val_dataset, batch_size=sets.batch_size, shuffle=True, num_workers=sets.num_workers,\n",
    "                                 pin_memory=sets.pin_memory)\n",
    "        for epoch in range(1, total_epochs + 1):\n",
    "            labels,output,loss = [],[],[]\n",
    "            labels,output,loss = train(train_data_loader,model, optimizer, scheduler,total_epochs, save_interval=sets.save_intervals,\n",
    "                                          save_folder=sets.save_folder, sets=sets)   \n",
    "            f1 = f1_score(labels, output, average='macro')\n",
    "            precision = precision_score(labels, output, average='macro')\n",
    "            recall = recall_score(labels, output, average='macro')\n",
    "            loss_train=sum(loss)/len(train_data_loader)\n",
    "            print('Train Epoch{}:f1: {:.6f}\\trecall: {:.6f}\\tprecision: {:.6f}\\tloss: {:.6f}'.format(epoch,f1,recall,precision,loss_train))\n",
    "            #labels_val,out_val,val_loss=[],[],[]\n",
    "            labels_val,out_val,val_loss = val(val_data_loader, model, sets)\n",
    "            f1_val = f1_score(labels_val,out_val, average='macro')\n",
    "            f1_val_list.append(f1_val)\n",
    "            precision_val = precision_score(labels_val,out_val, average='macro')\n",
    "            recall_val = recall_score(labels_val,out_val, average='macro')\n",
    "            loss_val=sum(val_loss)/len(val_data_loader)\n",
    "            print('Val Epoch:f1: {:.6f}\\trecall: {:.6f}\\tprecision: {:.6f}\\tloss: {:.6f}'.format(f1_val,recall_val,precision_val,loss_val))\n",
    "        \n",
    "            if f1_val == max(f1_val_list):    #那个列表下标有点问题\n",
    "                #if batch_id_sp != 0 and batch_id_sp % save_interval == 0:\n",
    "                    model_save_path = './trails/models/fold_{}.pth.tar'.format(i)\n",
    "                    model_save_dir = os.path.dirname(model_save_path)\n",
    "                    if not os.path.exists(model_save_dir):\n",
    "                        os.makedirs(model_save_dir)\n",
    "\n",
    "                    torch.save({\n",
    "                                'fold':i,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optimizer': optimizer.state_dict()},\n",
    "                                model_save_path)\n",
    "\n",
    "        sets.phase='test'\n",
    "        checkpoint=torch.load(model_save_path)\n",
    "        net,_=generate_model(sets)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        # data tensor\n",
    "        test_dataset = CTDataset(sets.data_root, sets.img_list_test, sets, phase='test')\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=sets.batch_size, shuffle=True, num_workers=sets.num_workers,pin_memory=sets.pin_memory)\n",
    "        #label_test,out_test=[],[]\n",
    "        label_test,out_test=test(test_data_loader,net,sets)\n",
    "        f1_test = f1_score(label_test, out_test, average='macro')\n",
    "        recall_test = recall_score(label_test, out_test, average='macro')\n",
    "        precision_test = precision_score(label_test, out_test, average='macro')\n",
    "        print('Test Epoch:f1: {:.6f}\\trecall: {:.6f}\\tprecision: {:.6f}'.format(f1_test,recall_test,precision_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c4988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "pytorch2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
